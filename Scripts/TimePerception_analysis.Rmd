---
title: "TimePerception_analysis"
author: "Barbora Ferusová"
date: "`r Sys.Date()`"
output: pdf_document
---

> Conditions: 1 is slow, 2 is normal, and 3 is fast Real timer durations: Slow:6 Normal:5 Fast:4

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/barbo/OneDrive/Počítač/3rd semester/PA/Project/PercAct-Time-Perception")
knitr::opts_knit$set(root.dir = normalizePath('C:/Users/barbo/OneDrive/Počítač/3rd semester/PA/Project/PercAct-Time-Perception'))

pacman::p_load(
  tidyverse,
  gridExtra, 
  dplyr, 
  ggplot2,
  readr,
  tibble,
  kableExtra,
  pastecs,
  car,
  WRS2,
  brms,
  rstanarm,
  patchwork,
  modelsummary
)

```

### Import logfiles and merge

```{r}
# List CSV files in the Logfiles folder
files <- list.files("Logfiles", pattern = "\\.csv$", full.names = TRUE)

# Read and combine all CSV files
df <- do.call(rbind, lapply(files, read.csv))

#delete the X column
df <- df[-1]
```

### Correct the time estimation

```{r}
# Reshape the data from wide to long format and select relevant columns
long_data <- pivot_longer(df, cols = starts_with(c("Beep_Time", "Button_Estimate", "Written_Estimate")), 
                          names_to = "Type", 
                          values_to = "Value")

# Extract the number from the Type column
long_data$Time <- gsub("\\D", "", long_data$Type)
long_data$Type <- gsub("\\d", "", long_data$Type)

# Reshape the data back to wide format and select relevant columns
reshaped_data <- pivot_wider(long_data, names_from = Type, values_from = Value)

# Combine reshaped data with the original columns
cleaned_df<- cbind(df[, -which(names(df) %in% grep("^Beep_Time|^Button_Estimate|^Written_Estimate", names(df)))], reshaped_data)

# Distinguish between tone estimation trials
cleaned_df$Time <- as.numeric(cleaned_df$Time)
cleaned_df <- cleaned_df %>%
  mutate(Phase = ifelse(Time >= 1 & Time <= 6, "baseline", "effect"))

write.csv(cleaned_df, "cleaned_data.csv", row.names = FALSE)

```

### Get survey data

```{r}
survey_df <- read_csv("Survey_responses.csv")
survey_df <- survey_df[-1]
```

### Merge PsychoPy and survey data

```{r}
merged_df <- merge(cleaned_df, survey_df, by = c("ID", "Condition"))
# Rename survey columns that will be used in analysis
merged_df <- merged_df %>% 
  rename(
    satisfaction = "How well do you think you performed in the lego building task?",
    enjoyment = "How much did you enjoy the lego building task?",
    stress = "How stressed were you during the lego building task?",
    difficulty = "How difficult was the lego building task?",
    focus = "How focused were you during the lego building task?",
    perceived_speed_of_time = "How fast did the time seem to be passing by during the lego building task?",
    tired = "How tiring was the lego building task?",
    attention_timer = "How much attention did you pay to the timer during the lego building task?",
    knows_sixbricks = "Where you familiar with the lego house six bricks challange, before the experiment?",
    lego_exp = "How much experience did you have with building legos in the past?",
    perceived_time = "What was the duration of the lego task?",
    action = "During the tone estimation task, how did you estimate its time?",
    models_done = "Number of lego models done",
    models_correct = "Number of lego models done correctly",
    written_estimate = "Written_Estimate_",
    button_estimate = "Button_Estimate_",
    beep_time = "Beep_Time"
  )
```

### Some more cleaning

```{r}
# set NAs in lego experience to 0
merged_df$lego_exp[is.na(merged_df$lego_exp)] <- 0

# delete characters from perceived_time
merged_df$perceived_time <- gsub("[^0-9]", "", merged_df$perceived_time)

merged_df$Phase <- as.factor(merged_df$Phase)
merged_df$Time <- as.factor(merged_df$Time)
merged_df$ID <- as.factor(merged_df$ID)
merged_df$Gender <- as.factor(merged_df$Gender)
merged_df$Condition <- as.factor(merged_df$Condition)
merged_df$button_estimate <- as.numeric(merged_df$button_estimate)
merged_df$action <- as.character(merged_df$action)


```

### Deal with faulty data points using mean imputation

```{r}
# 1. participant 13, button estimate 7
# Calculate the mean
button7_cond1_mean <- merged_df %>% 
  filter(ID != 13, Time == 7, Condition == 1) %>% 
  summarise(mean_value = mean(button_estimate, na.rm = TRUE)) %>%
  pull(mean_value)

# Identify the row and replace the value
row_to_change <- which(merged_df$ID == 13 & merged_df$Time == 7)
merged_df$button_estimate[row_to_change] <- button7_cond1_mean

# 2. participant 5, button 1, same method
# Calculate the mean
button1_mean <- merged_df %>% 
  filter(ID != 5, Time == 1) %>% 
  summarise(mean_value = mean(button_estimate, na.rm = TRUE)) %>%
  pull(mean_value)

# Identify the row and replace the value
row_to_change <- which(merged_df$ID == 5 & merged_df$Time == 1)
merged_df$button_estimate[row_to_change] <- button1_mean

```

### Calculate other needed metrics like PRODUCTIVITY and BUTTON ACCURACY

```{r}
# make the "productivity" variable which is models_done/300seconds for Condition = 2
merged_df <- merged_df %>%
  mutate(productivity = case_when(
    Condition == 2 ~ models_done / 300,
    Condition == 1 ~ models_done / 360,
    Condition == 3 ~ models_done / 240
  ))

# make an estimation_effect that shows the difference in time perceptions where negative values mean underestimation (for example in fast condition) and positive mean overestimation (for example in slow condition)
merged_df <- merged_df %>%
  mutate(button_accuracy = button_estimate - beep_time)
merged_df <- merged_df %>%
  mutate(written_accuracy = written_estimate - beep_time)
merged_df$button_accuracy <- as.numeric(merged_df$button_accuracy)

# beep time higher than estimate => negative value, thus underestimation
# I swear I have to write this down over and over cuz I keep forgetting
```

### Handling the action variable

```{r}
# 1. Split and sort options
merged_df <- merged_df %>%
  mutate(action_sorted = sapply(strsplit(as.character(action), ", "), function(x) paste(sort(x), collapse = ", ")))

# 2. Create a mapping from sorted actions to unique numbers
unique_actions <- unique(merged_df$action_sorted)
action_mapping <- setNames(seq_along(unique_actions), unique_actions)

# 3. Apply the mapping to create a numeric variable
merged_df <- merged_df %>%
  mutate(action_numeric = action_mapping[action_sorted])

# CREATE NICE LEGEND
# Generate counts for each action_numeric
actions_counted <- merged_df %>%
  filter(Time==1) %>% 
  group_by(action_numeric) %>%
  summarise(count = n())

# Create a table with unique action_numeric and corresponding action_sorted
distinct_actions <- merged_df %>%
  select(action_numeric, action_sorted) %>%
  distinct() %>% 
  rename(`Estimation Method Number` = action_numeric, `Estimation Method` = action_sorted)


# Merge the counts with the distinct actions
legend_table <- left_join(distinct_actions, actions_counted, by = c( "Estimation Method Number" = "action_numeric"))

kable_table <- kable(legend_table, format = "html", caption = "Legend of Actions with Counts") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
kable_table
```

## Exploratory phase

```{r exploratory}
# number of participants of each gender by condition
cond1 <- merged_df %>% 
  filter(Condition ==1) %>% 
  group_by(Gender) %>% 
  distinct(ID) %>% 
  count() %>% 
  mutate(Condition = 1)

cond2<-merged_df %>% 
  filter(Condition ==2) %>% 
  group_by(Gender) %>% 
  distinct(ID) %>% 
  count() %>% 
  mutate(Condition = 2)

cond3<-merged_df %>% 
  filter(Condition ==3) %>% 
  group_by(Gender) %>% 
  distinct(ID) %>% 
  count() %>% 
  mutate(Condition = 3)

bind_rows(cond1, cond2, cond3)

# mean productivity by gender in control
merged_df %>% 
  filter(Condition==2) %>% 
  group_by(Gender) %>% 
  summarise(mean(productivity))
# mean procudtivity by condition
merged_df %>% 
  group_by(Condition) %>% 
  summarise(mean(productivity))
```

> I need to note that the Fast condition is mostly composed by Female participants. In the control condition, Male participants were slightly more productive. Productivity in the Fast condition jumped by \~0.049 from the control. While it only increased by \~0.004 in the slow condition where participants had more time.

### After being exposed to a sped/slowed down up timer while performing a lego recreation task, participants will estimate tone durations as shorter/longer.

```{r button estimation}
# average difference between baseline and effect estimates 

# Calculate mean baseline accuracy for each ID
baseline_df <- merged_df %>% 
  filter(Phase == "baseline") %>% 
  group_by(ID) %>% 
  summarise(mean_baseline_accuracy = mean(button_accuracy, na.rm = TRUE))

# Calculate mean effect accuracy for each ID
effect_df <- merged_df %>% 
  filter(Phase == "effect") %>% 
  group_by(ID) %>% 
  summarise(mean_effect_accuracy = mean(button_accuracy, na.rm = TRUE))

# Join and calculate the experimental effect
effect_vs_baseline <- baseline_df %>%
  left_join(effect_df, by = "ID") %>%
  mutate(experimental_effect = abs(mean_baseline_accuracy - mean_effect_accuracy))

# Join this information back to the original merged_df
# Ensure only necessary columns are included to avoid duplications
merged_df <- merged_df %>%
  left_join(select(effect_vs_baseline, ID, mean_baseline_accuracy, mean_effect_accuracy, experimental_effect), by = "ID")

# now we have average accuracy for both baseline and effect for each ID, meaning we have two averages for each ID
# now we need the average difference across conditions

merged_df %>% 
  group_by(Condition)%>% 
  summarise(mean(mean_baseline_accuracy), mean(mean_effect_accuracy), mean(experimental_effect)) 
# lower value means more they underestimated

```

> Slow condition underestimated after the lego task (perceived the tune to last shorter), but not by a lot. Fast condition underestimated after the lego task (perceived the tune to last shorter), by a lot.

```{r written estimation}
# Calculate mean baseline accuracy for each ID
baseline_df <- merged_df %>% 
  filter(Phase == "baseline") %>% 
  group_by(ID) %>% 
  summarise(mean_baseline_accuracy = mean(written_accuracy, na.rm = TRUE))

# Calculate mean effect accuracy for each ID
effect_df <- merged_df %>% 
  filter(Phase == "effect") %>% 
  group_by(ID) %>% 
  summarise(mean_effect_accuracy = mean(written_accuracy, na.rm = TRUE))

# Join and calculate the experimental effect
effect_vs_baseline <- baseline_df %>%
  left_join(effect_df, by = "ID") %>%
  mutate(experimental_effect = abs(mean_baseline_accuracy - mean_effect_accuracy))

# Join this information back to the original merged_df
# Ensure only necessary columns are included to avoid duplications
merged_df <- merged_df %>%
  left_join(select(effect_vs_baseline, ID, mean_baseline_accuracy, mean_effect_accuracy, experimental_effect), by = "ID")

# now we have average accuracy for both baseline and effect for each ID, meaning we have two averages for each ID
# now we need the average difference across conditions

merged_df %>% 
  group_by(Condition)%>% 
  summarise(mean(mean_baseline_accuracy), mean(mean_effect_accuracy), mean(experimental_effect)) 
# lower value means more they underestimated

```

> In written estimates: slow condition overestimated more and fast condtion overestimated less. Big effects seen on slow and control, not fast.

```{r save csv merged}
write.csv(merged_df, "merged_data.csv", row.names = FALSE)
```

## Plots

```{r histograms button accuracy}
# plotting the distribution of time perceptions through button estimates
merged_df %>% 
  filter(Phase == "baseline") %>% 
ggplot(aes(x = button_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates before Lego Task", x = "Button Estimate", y = "Frequency")

merged_df %>% 
  filter(Phase == "effect" | Condition == 1) %>% 
ggplot(aes(x = button_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Slowed Timer", x = "Button Estimate", y = "Frequency")

merged_df %>% 
  filter(Phase == "effect" | Condition == 3) %>% 
ggplot(aes(x = button_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Sped up Timer", x = "Button Estimate", y = "Frequency")


merged_df %>% 
  filter(Phase == "effect" | Condition == 2) %>% 
ggplot(aes(x = button_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Regular Timer", x = "Button Estimate", y = "Frequency")

```

```{r histograms written accuracy}

# plotting the distribution of time perceptions through written estimates
merged_df %>% 
  filter(Phase == "baseline") %>% 
ggplot(aes(x = written_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates before Lego Task", x = "Button Estimate", y = "Frequency")

merged_df %>% 
  filter(Phase == "effect" | Condition == 1) %>% 
ggplot(aes(x = written_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Slowed Timer", x = "Button Estimate", y = "Frequency")

merged_df %>% 
  filter(Phase == "effect" | Condition == 3) %>% 
ggplot(aes(x = written_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Sped up Timer", x = "Button Estimate", y = "Frequency")


merged_df %>% 
  filter(Phase == "effect" | Condition == 2) %>% 
ggplot(aes(x = written_accuracy)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Button Estimates after Regular Timer", x = "Button Estimate", y = "Frequency")

```

### Participants that were presented with a rigged timer will estimate tone durations less accurately than before the lego task.

> Yes. In button estimate slow and fast were -0.66 off on average while control was only -0.42 off. No. In written estimate slow and fast was 2.01 and 0.25 and control was 0.83. Meaning in this case the fast condition was more accurate.

```{r before vs after task by condition}
# plot the difference between button and written estimation perception in baseline AND effect trials
merged_df %>% 
  filter(Condition == 1) %>% 
ggplot(aes(x = factor(Phase), y = button_accuracy, fill = Phase)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Button Estimate before and after in Slow Condition", x = "Phase", y = "Button Estimate")

merged_df %>% 
  filter(Condition == 3) %>% 
ggplot(aes(x = factor(Phase), y = button_accuracy, fill = Phase)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Button Estimate before and after in Fast Condition", x = "Phase", y = "Button Estimate")

merged_df %>% 
  filter(Condition == 2) %>% 
ggplot(aes(x = factor(Phase), y = button_accuracy, fill = Phase)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Button Estimate before and after in Normal Condition", x = "Phase", y = "Button Estimate")
```

> It seems like in all conditions, after completing the Lego Task participants tend to underestimate the tune duration. This could be because they forget to count towards the end or start counting too late into the tone.

```{r estimate accuracy after task by condition}
merged_df %>% 
  filter(Phase=="effect") %>% 
ggplot( aes(x = factor(Condition), y = button_accuracy, fill = Condition)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Button Estimate by Condition after Lego Task", x = "Condition", y = "Button Estimate")

```

### While being exposed to a sped/slowed down up timer when they're performing a lego recreation task, participants will be more/less productive (the ratio of completed configurations and actual time will be higher than the control condition)

```{r productivity by condition}
merged_df %>% 
ggplot( aes(x = factor(Condition), y = productivity, fill = Condition)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Productivity by Condition", x = "Condition", y = "Productivity")

ggplot(merged_df, aes(x = productivity, y = stress)) +
  geom_point(aes(color = Condition), alpha = 0.7, position = position_jitter(width = 0.1, height = 0.1)) +
  theme_minimal() +
  labs(title = "Productivity vs. Stress", x = "Productivity", y = "Stress")

```

> Big diffference in productivity in the Fast condition. Seems like they did proportionately better. Slow might be better than condition just because they objectively had more time.

### While being exposed to a sped up/slowed down timer when they're performing a lego recreation task, participants will report feeling more/less focused on the task.

```{r focus by condition}
merged_df %>% 
  group_by(Condition) %>% 
  summarise(mean(focus))
```

> Focus reported was highest in the slow condition, second highest in the fast condition.

### While being exposed to a sped/slowed down up timer when they're performing a lego recreation task, participants will report being less/more stressed during the task.

```{r stress by condition}
merged_df %>% 
  group_by(Condition) %>% 
  summarise(mean(stress))
```

> Oddly, the control group was the most stressed, closely followed by the fast group. The slow group was the least stressed, probably after noticing their timer.

### How many noticed the rigged timer?

```{r}
merged_df$perceived_time <- as.factor(merged_df$perceived_time)
incorrect_estimates <- merged_df %>%
  filter(Time==1) %>% 
  mutate(Incorrect_Estimation = case_when(
    Condition == 1 & perceived_time != 6 ~ TRUE,
    Condition == 2 & perceived_time != 5 ~ TRUE,
    Condition == 3 & perceived_time != 4 ~ TRUE,
    TRUE ~ FALSE
  )) %>%
  group_by(Condition) %>%
  summarise(Count_Incorrect = sum(Incorrect_Estimation, na.rm = TRUE))

incorrect_estimates
```

> Out of 20 people in the rigged conditions, staggering 18 of them noticed the timer was rigged.

### How many paid attention to the timer?

```{r}
merged_df %>% 
  filter(Time==1) %>% 
  group_by(Condition) %>% 
  summarise(mean(attention_timer)) 
```

> The slow condition was supposedly double-checking why the timer wasn't running out as they expected. Fast condition didn't have time to notice / they didn't expect the timer to run out. Overall not many reported being too aware of the timer which can explain variability in productivity and button_accuracy after the task.

### Which action brings more accurate estimates?

```{r estimate accuracy by action}
merged_df$action_numeric <- as.factor(merged_df$action_numeric )
action_means<- merged_df %>% 
  group_by(action_numeric) %>% 
  summarise(mean_estimation = mean(button_accuracy))

ggplot(action_means, aes(x = action_numeric, y = mean_estimation)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_x_discrete(drop = T) +  # Ensure all x-axis labels are shown
  theme_minimal() +
  labs(x = "Action Numeric", y = "Mean Estimation Effect Button", 
       title = "Mean Estimation Effect Button by Action Number")
```

> Participants (n=2) using 5 (reimagining the sound in your head) tend to underestimate and be very inaccurate Participant using 10 (tapping along) overestimated the duration and was fairly innacurate. 6 and 2 (using mixed methods) is most accurate but both only used by one participant The most used method, 1 or counting in your mind, was moderately accurate

### Comparing genders

```{r}
merged_df %>% 
ggplot(aes(x = factor(Condition), y = productivity, fill = Gender)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Productivity by Condition and Gender", x = "Condition", y = "Productivity") +
  facet_wrap(merged_df$Gender)
```

# NORMALITY TESTING

```{r testing button_estimate}
# Q-Q plot 
norm <- merged_df %>% ggplot(aes(x = button_accuracy)) +
  geom_histogram(aes(y = after_stat(density)), 
                 binwidth = 0.5, 
                 color = "black", 
                 fill = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(merged_df$button_accuracy, na.rm = TRUE),
                sd = sd(merged_df$button_accuracy, na.rm = TRUE)), 
                colour = "darkgreen", linewidth = 1) +
  theme_classic() + 
  xlim(range(merged_df$button_accuracy)) +
  ggtitle("button_accuracy Normal Distribution")


 button_accuracy_qq <- ggplot(data = merged_df, aes(sample = button_accuracy)) +
    stat_qq()+
    stat_qq_line()+
    labs(x = "Theoretical quantiles", y = "Sample quantiles")+
   theme_minimal()
 
 grid.arrange(norm, button_accuracy_qq, ncol=2)

shapiro.test(merged_df$button_accuracy) # also <.05


button_accuracy <- merged_df %>% 
  select(button_accuracy)
normality <-  round(stat.desc(button_accuracy, basic=FALSE, norm = TRUE), digits = 2)

```

```{r testing written estimate}
# Q-Q plot
norm <- merged_df %>% ggplot(aes(x = written_accuracy)) +
  geom_histogram(aes(y = after_stat(density)), 
                 binwidth = 1, 
                 color = "black", 
                 fill = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(merged_df$written_accuracy, na.rm = TRUE),
                sd = sd(merged_df$written_accuracy, na.rm = TRUE)), 
                colour = "darkgreen", linewidth = 1) +
  theme_classic() + 
  xlim(range(merged_df$written_accuracy)) +
  ggtitle("written_accuracy Normal Distribution")


 written_accuracy_qq <- ggplot(data = merged_df, aes(sample = written_accuracy)) +
    stat_qq()+
    stat_qq_line()+
    labs(x = "Theoretical quantiles", y = "Sample quantiles")+
   theme_minimal()
 
 grid.arrange(norm, written_accuracy_qq, ncol=2)

shapiro.test(merged_df$written_accuracy) # also <.05

shapiro.test(merged_df$experimental_effect) # >.05 can be used

```

> Can't reject normality.

## Homogeneity of Variances

```{r}
# Levene's Test for 'button_estimate' across different 'Condition'
leveneTest(button_estimate ~ Condition, data = merged_df)
# you don't need to meet the homogeneity assumption if the groups you're comparing have roughly equal sample sizes
# >.05, the variances are not significantly different from each other (i.e., the homogeneity assumption of the variance is met)
fligner.test(button_accuracy ~ Condition, merged_df) # equal variances
```

## Transforming

```{r}
accuracy_trans <- merged_df %>%  select(c(ID, button_accuracy)) %>%  mutate(log = log(button_accuracy), sqrt = sqrt(button_accuracy), reciprocal = 1/button_accuracy)
shapiro.test(accuracy_trans$log)
shapiro.test(accuracy_trans$sqrt) # >.05 could be considered normal
shapiro.test(accuracy_trans$reciprocal)
```

## Correlation

```{r}
merged_df$action_numeric <- as.factor(merged_df$action_numeric)
merged_df %>% 
  select(c(ID, Age, Gender, Condition, Phase,experimental_effect, productivity, stress, action_numeric)) %>% 
  plot()
```

```{r correlation}
ggplot(merged_df,aes(x = productivity , y = experimental_effect))+
  geom_point()+
  geom_smooth(method=lm)+
  theme_minimal()+
      labs(x = "Productivity", y = "Time Perception")
# the more productive the more accurate?
 ggplot(merged_df,aes(x = stress , y = experimental_effect))+
  geom_point()+
  geom_smooth(method=lm)+
  theme_minimal()+
      labs(x = "Stress", y = "Time Perception")
 
```

```{r correlation}
# experimental effect ~ stress
cor.test(merged_df$experimental_effect, merged_df$stress, method = 'kendall') # significant + positive
cor.test(merged_df$experimental_effect, merged_df$stress, method = 'spearman') # significant + positive

# experimental effect ~ focus
cor.test(merged_df$experimental_effect, merged_df$focus, method = 'kendall') # significant + negative
cor.test(merged_df$experimental_effect, merged_df$focus, method = 'spearman') # significant + negative
```

> We found a significant and positive relationship between experimental_effect (i.e. how different the estimates were before vs after the task) and stress during the lego task. \> So the more stressed you were during the task the more inaccurate your estimates after. We found a significant and negative relationship between experimental_effect (i.e. how different the estimates were before vs after the task) and focus during the lego task. \> So the more focused you were the less inaccurate your estimates after

```{r correlation for condition}
WRS2::yuen(experimental_effect ~ Condition, 
                          data = merged_df, trim = 0.2) # <.05 sig. relationship
WRS2::yuen(productivity ~ Condition, data = merged_df, trim = 0.2) # <.05 sig. relationship
WRS2::yuen(button_accuracy ~ Condition, data = merged_df, trim = 0.2) # >.05 not sig.
WRS2::yuen(stress ~ Condition, data = merged_df, trim = 0.2) # <.05 sig. relationship
WRS2::yuen(focus ~ Condition, data = merged_df, trim = 0.2) # >.05 not sig.
```

> The Yuen test uncovered a significant difference between conditions in experimental_effect and productivity.

## Mixed-effects model

```{r}
model1 <- lme4::glmer(experimental_effect ~ Condition + Age + Gender + stress + focus + (1|ID), 
               data = merged_df, 
               family = gaussian)
plot(model1)
summary(model1)

MuMIn::r.squaredGLMM(model1) # R2 is 0.83


model2 <- lme4::glmer(experimental_effect ~ Condition*stress + Age + Gender  + focus + (1|ID), 
               data = merged_df, 
               family = gaussian)
MuMIn::r.squaredGLMM(model2) # R2 is 0.88

```

### Residuals

```{r}
plot(fitted(model2), residuals(model2), xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

qqnorm(residuals(model2))
qqline(residuals(model2))

hist(residuals(model2), breaks = "Sturges", main = "Histogram of Residuals", xlab = "Residuals")
# Or for a density plot
plot(density(residuals(model1)), main = "Density Plot of Residuals", xlab = "Residuals")

plot(model2, which = 5)

```

```{r}
shapiro.test(residuals(model2))
```

# Bayesian modeling using baseline as priors

> SPOILERS: it didnt work

### Define priors

```{r}
baseline_data <- merged_df %>% filter(Phase == "baseline")

model_formula <- bf(button_accuracy ~ Condition + Age + Gender + stress + focus + (1|ID))

priors <- prior(normal(mean(baseline_data$button_accuracy, na.rm = TRUE), 
              sd(baseline_data$button_accuracy, na.rm = TRUE)), class = "b")

fit_model <- brm(model_formula, 
                 data = merged_df %>% filter(Phase == "effect"),
                 prior = priors,
                 family = gaussian(), 
                 data2 = list(N = nrow(data)),
                 chains = 4, 
                 iter = 2000,
                 control = list(adapt_delta = 0.95))

```

### Examine results

```{r}
# Summary of the model
summary(fit_model)
# Diagnostics
plot(fit_model)
```

### Posterior Predicts Checks

```{r}
pp_check(model_fit)
```
